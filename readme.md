# E-RLAIF: Ensemble-RLAIF for Clinical Note Generation

[![Python 3.11](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/downloads/release/python-3110/)
[![Unsloth](https://img.shields.io/badge/Framework-Unsloth-ff69b4)](https://pytorch.org/)
[![Hugging Face](https://img.shields.io/badge/Model-%F0%9F%A4%97%20Hugging%20Face-blue)](https://huggingface.co/The-Welcomer/cluster-test-unbiased)
[![License: Apache 2.0](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

This repository contains the official implementation for **E-RLAIF (Ensemble-Reinforcement Learning from AI Feedback)**, a novel framework for fine-tuning Large Language Models (LLMs) on complex linguistic tasks with abstract reward criteria.

The primary application demonstrated here is the training of **Clinician Note**, an 8-billion parameter AI scribe that generates high-quality clinical SOAP notes from doctor-patient dialogue transcripts. This work aims to address the significant challenge of physician burnout caused by extensive clinical documentation by providing a robust, AI-powered solution.

## Abstract

Supervised fine-tuning (SFT) and transformer reinforcement learning (TRL) are effective techniques for model alignment, yet their scope remains constrained. In particular, TRL is limited by the difficulty of defining reward functions for unverifiable rewards, especially for linguistic tasks like clinical note generation. To address these challenges, we introduce **Ensemble-RLAIF (E-RLAIF)**, an extension of RLAIF which uses an ensemble of LLM judges for model fine-tuning. E-RLAIF introduces two reward signals—one derived from comparisons with a ground truth and another from comparison with a note from the current generator model—and an EMA Z-Score normalization regime that significantly improves training stability. We use E-RLAIF to train **Clinician Note**, an 8B parameter AI scribe that demonstrates state-of-the-art performance in generating clinically relevant documentation.

## The E-RLAIF Framework

E-RLAIF enhances traditional RLAIF by incorporating three key innovations to improve training stability and model performance on tasks with subjective rewards.

<p align="center">
  <img src="images/E-RLAIF Architecture.png" alt="E-RLAIF Architecture" width="800"/>
  <br/>
  <em>Figure 1: The E-RLAIF Architecture.</em>
</p>

<p align="center">
  <img src="images/LLM Judge Ensemble.png" alt="LLM Judge Ensemble" width="800"/>
  <br/>
  <em>Figure 2: LLM Judge Ensemble</em>
</p>

### Key Innovations

1. **Ensemble of LLM Judges**: Instead of relying on a single AI evaluator, E-RLAIF uses a diverse ensemble of powerful LLMs to score generated outputs. This mitigates individual model biases and produces a more robust and reliable preference signal. The current implementation uses:
   - `nvidia/llama-3.3-nemotron-super-49b-v1`
   - `deepseek/deepseek-chat-v3-0324`
   - `google/gemini-2.5-flash-preview-05-20`

2. **Dual Reward Signals**: The framework calculates two parallel reward signals to guide the training process:
   - **R_base (Base Reward)**: Compares each generated note against a static, high-quality "ground truth" reference note (e.g., generated by Gemini 2.5 Pro). This anchors the model to a consistent quality standard.
   - **R_ft (Fine-Tuning Reward)**: Compares each generated note against a baseline note generated by the policy model's current LoRA weights. This creates a dynamic, adaptive signal that encourages rapid self-improvement.

3. **Two-Stage EMA Z-Score Normalization**: To stabilize training, a robust normalization strategy is applied:
   - **Stage 1 (Per-Judge)**: The scores from each individual judge are normalized using a running Exponential Moving Average (EMA) of that judge's historical mean and variance.
   - **Stage 2 (Per-Signal)**: The aggregated scores for R_base and R_ft are normalized again using a separate set of EMAs. This ensures that the final reward signals fed to the optimizer are stable and well-distributed.

This combination allows for effective fine-tuning on complex, unlabeled datasets where defining a simple reward function is intractable.

## Testing Results

**Clinician Note at 4-bit quantization outperforms Qwen3 235B A22B, a model more than 50 times its size.** Below is a histogram showing the performance of Clinician Note in side-by-side evaluations with Qwen3 235B by GPT 4.1. The scores range from 1 to -1, with 1 indicating the note generated by Clinician Note significantly outformed Qwen 3 235B and -1 indicating the converse:

<p align="center">
  <img src="images/Average Reward Distribution.png" alt="LLM Judge Ensemble" width="800"/>
  <br/>
  <em>Figure 3: Reward Distribution of Clinician Note vs Qwen3 235B side-by-side comparisons by GPT 4.1.</em>
</p>

## Repository Structure

```
.
├── environment.yml             # Conda environment with all dependencies
├── submit_grpo_job.sh          # SLURM script for submitting training jobs on an HPC cluster
├── trainer.py                  # The main E-RLAIF training script using GRPO
├── paper/                      # Directory containing the research paper LaTeX and .pdf
├── base-vs-ft.csv/             # .csv containing 99 test dataset transcripts and their corresponding notes from Clinician Note and its base model (Qwen3 8B).  
├── demo.ipynb/                 # Demo notebook which generates a note from the base and from the Clinician Note models for side-by-side comparison
├── grpo_training/              # (Generated) Output directory for models, checkpoints, and LoRA adapters
├── logs/                       # (Generated) Directory for SLURM output and error logs
├── env.example/                # .env template to create a .env from 
└── README.md                   # This file
```

### Compare Clinician Note to its Base Model
We recommend either running the demo or uploading base-vs-ft.csv to Excel or Google Sheets where you can compare the notes for yourself!

## Run Demo
We recommend you run the demo.ipynb locally or on Google Colab by uploading it and pressing Run All. You can change the note style and transcript used, or add your own! The demo takes approximately 24 GB of vram to run, so unless you have 32 GB of vram or unified RAM on your computer, we recommend running it on Google Colab using a GPU with 24 GB or more of vram.

## Run Training

### Prerequisites

- NVIDIA GPU with CUDA 12.1 or later. An A100/H100 with 80GB VRAM is recommended.
- [Conda](https://docs.conda.io/en/latest/miniconda.html) or [Mamba](https://mamba.readthedocs.io/en/latest/installation.html) for environment management.
- Git.

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/TheWelcomer/Ensemble-RLAIF.git
   cd Ensemble-RLAIF
   ```

2. **Create and activate the Conda environment:**
   This will install all necessary dependencies from `environment.yml`.

   ```bash
   conda env create -f environment.yml
   ```

   Then, activate the environment:
   ```bash
   conda activate grpo-training
   ```

3. **Set up environment variables:**
   The training script requires API keys for Hugging Face, Weights & Biases, and the LLM evaluators.

   Create a `.env` file in the root of the repository:
   ```bash
   cp .env.example .env
   ```

   Now, edit the `.env` file and add your secret keys:
   ```dotenv
   # .env
   # Hugging Face API Key (for pushing models to the Hub)
   HF_API_KEY="hf_..."

   # Weights & Biases API Key (for experiment tracking)
   WANDB_API_KEY="..."

   # API Keys for Evaluator Models
   GEMINI_API_KEY="..."
   OPENROUTER_API_KEY="..."
   ```

## Training

The training is orchestrated by `trainer.py`, which uses the `trl` library's `GRPOTrainer` and `unsloth` for high-performance, memory-efficient fine-tuning.

### Running on an HPC Cluster (SLURM)

The `submit_grpo_job.sh` script is designed for submitting jobs to a SLURM-managed cluster. It is well-documented and includes robust checks for CUDA and Conda environments.

1. **Customize the SLURM script:**
   Before submitting, you may need to adjust `submit_grpo_job.sh` for your cluster's configuration:
   - `--partition`: Set to the available GPU partitions on your cluster.
   - `--constraint`: Specify the GPU type (e.g., "a100", "h100").
   - `CONDA_BASE` and `CONDA_ENVS_PATH`: Update these paths to match your cluster's Conda installation if they differ.

2. **Submit the job:**
   ```bash
   sbatch submit_grpo_job.sh
   ```
   Logs will be saved to the `logs/` directory, and model artifacts (checkpoints, LoRA adapters) will be saved to `grpo_training/`.

### Running on Google Colab

To run on google colab, upload trainer.ipynb, open the jupyter notebook, and click Run All. We highly recommend running on an A100 GPU or you will most likely run out of memory. If this is not possible, consider using Qwen 3 1B as the base model. Note that the .ipynb fine tunes Qwen 3 4B by default while trainer.py fine tunes Qwen 3 8B.

### Running Locally (for Debugging)

While the full training run is resource-intensive, you can run the script locally on a capable machine for debugging purposes.

1. **Execute the script:**
   Ensure your `.env` file is configured and the `grpo-training` conda environment is active.
   ```bash
   python trainer.py
   ```
   If you don't have the required memory, we recommend changing the base model to Qwen3 4B or 1B or running the .ipynb on Google Colab with an A100 GPU.

## Key Components

- **Base Model**: [unsloth/Qwen3-8B](https://huggingface.co/unsloth/Qwen3-8B-Instruct-bnb-4bit) (Qwen3-4B if using `trainer.ipynb`) is used as the starting point for fine-tuning.
- **Training Dataset**: The model is trained on a custom dataset of doctor-patient dialogues, available at [The-Welcomer/scribing-train-dataset-batched](https://huggingface.co/datasets/The-Welcomer/scribing-train-dataset-batched).
- **Final Model**: The fine-tuned model, **Clinician Note**, is available on the Hugging Face Hub at [The-Welcomer/Clinician-Note](https://huggingface.co/The-Welcomer/Clinician-Note).

## Citation

If you use E-RLAIF or the Clinician Note model in your research, please cite our work:

```bibtex
@misc{winkelman2025erlaif,
      title={E-RLAIF: Fine-Tuning LLMs with Ensemble-RLAIF for Abstract Linguistic Tasks}, 
      author={Donald Winkelman},
      year={2025},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://github.com/TheWelcomer/Ensemble-RLAIF}
}
```

## Special Thanks
I would like to thank Taoran Ye, Abigail Winkelman, Donald Winkelman, Michael Yingbull, Alex Simko, Nathan Wolf, Mitchell Sylvia, Maxwell Tang, Josh Leroy, Prakriti Shetty, Priya Balakrishnan, and Avinash Nandyala for making this project possible. I would additionally like to thank the ClinicianFOCUS project and MassAI community for their support throughout this project.